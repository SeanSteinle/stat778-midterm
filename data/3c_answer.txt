
    The cross-validation estimations of error for each model are as follows: {'Logistic': {'RKF': 0.16100000000000006, 'LOO': 0.155, 'GCV': 0.0800250000000004}, 'Linear': {'RKF': 0.13361685125018938, 'LOO': 0.1311063166609903, 'GCV': 0.11783555087979725}}.
    The conditional and unconditional estimations of error for each model are as follows: {'Logistic': {'Conditional': 0.15856754000064785, 'Unconditional': 0.14842912939369546}, 'Linear': {'Conditional': 0.1699, 'Unconditional': 0.16899360000000013}}.

    In both cases, the conditional error estimation is higher than the unconditional error estimation. In the case
    of logistic regression the conditional estimation is more similar to the majority of the cross-validation metrics.
    Interestingly, opposite is true for linear regression: unconditional estimation is more similar to the 
    cross-validation estimates.

    From a theoretical standpoint the conditional error estimation provides the best estimation of error for the
    entirety of the distribution because it incorporates a new portion of the distribution in its error estimate
    after training whereas cross-validation methods estimate error by splitting the training set many ways.
    